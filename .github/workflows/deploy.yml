name: CDK S3 Lambda Module Workflow

on:
  push:
    branches: [ add_actions ]
  pull_request:
    branches: [ main ]
  workflow_dispatch:
    # Allows manual triggering

permissions:
  contents: read
  id-token: write # Required for requesting the JWT

jobs:
  setup:
    name: Setup Environment
    runs-on: ubuntu-latest
    outputs:
      account_id: ${{ steps.aws-account.outputs.account_id }}
    steps:
      - name: Checkout code
        uses: actions/checkout@v3

      - name: Setup Node.js
        uses: actions/setup-node@v3
        with:
          node-version: '18'

      - name: Setup Python
        uses: actions/setup-python@v4
        with:
          python-version: '3.12'
          cache: 'pip'

      - name: Install dependencies
        run: |
          python -m pip install --upgrade pip
          pip install -r requirements.txt
          pip install -r requirements-dev.txt
          pip install aws-cdk-lib
          npm install -g aws-cdk  # Install the CDK CLI globally
          pip install awscli      # Install AWS CLI for S3 operations and log checking

      - name: Configure AWS credentials
        uses: aws-actions/configure-aws-credentials@v2
        with:
          aws-access-key-id: ${{ secrets.AWS_ACCESS_KEY_ID }}
          aws-secret-access-key: ${{ secrets.AWS_SECRET_ACCESS_KEY }}
          aws-region: ${{ secrets.AWS_REGION }}

      - name: Get AWS Account ID
        id: aws-account
        run: |
          AWS_ACCOUNT_ID=$(aws sts get-caller-identity --query "Account" --output text)
          echo "account_id=$AWS_ACCOUNT_ID" >> $GITHUB_OUTPUT
          echo "Using AWS Account: $AWS_ACCOUNT_ID"

      - name: CDK Bootstrap
        run: cdk bootstrap aws://${{ steps.aws-account.outputs.account_id }}/${{ secrets.AWS_REGION }}

  deploy:
    name: Deploy CDK Stack
    needs: setup
    runs-on: ubuntu-latest
    outputs:
      bucket_name: ${{ steps.stack-outputs.outputs.bucket_name }}
      lambda_name: ${{ steps.stack-outputs.outputs.lambda_name }}
    if: github.event_name == 'push' || github.event_name == 'workflow_dispatch'
    steps:
      - name: Checkout code
        uses: actions/checkout@v3

      - name: Setup Node.js and Python
        uses: actions/setup-node@v3
        with:
          node-version: '18'
      
      - name: Setup Python
        uses: actions/setup-python@v4
        with:
          python-version: '3.12'

      - name: Install dependencies
        run: |
          python -m pip install --upgrade pip
          pip install -r requirements.txt
          npm install -g aws-cdk
          pip install awscli

      - name: Configure AWS credentials
        uses: aws-actions/configure-aws-credentials@v2
        with:
          aws-access-key-id: ${{ secrets.AWS_ACCESS_KEY_ID }}
          aws-secret-access-key: ${{ secrets.AWS_SECRET_ACCESS_KEY }}
          aws-region: ${{ secrets.AWS_REGION }}

      - name: CDK Deploy
        run: cdk deploy --require-approval never --outputs-file cdk-outputs.json

      - name: Extract Stack Outputs
        id: stack-outputs
        run: |
          if [ ! -f cdk-outputs.json ]; then
            echo "cdk-outputs.json file not found!"
            ls -la
            exit 1
          fi
          
          # Display the full content of the outputs file for debugging
          echo "CDK Outputs file content:"
          cat cdk-outputs.json
          
          # Extract the values with error handling
          BUCKET_NAME=$(cat cdk-outputs.json | jq -r '.CdkS3LambdaStack.BucketName // empty')
          LAMBDA_NAME=$(cat cdk-outputs.json | jq -r '.CdkS3LambdaStack.LambdaFunction // empty')
          
          # Check if values were extracted
          if [ -z "$BUCKET_NAME" ] || [ -z "$LAMBDA_NAME" ]; then
            echo "Failed to extract values from outputs file"
            exit 1
          fi
          
          echo "bucket_name=$BUCKET_NAME" >> $GITHUB_OUTPUT
          echo "lambda_name=$LAMBDA_NAME" >> $GITHUB_OUTPUT
          echo "Bucket Name: $BUCKET_NAME"
          echo "Lambda Function: $LAMBDA_NAME"

  verify:
    name: Verify Deployment
    needs: deploy
    runs-on: ubuntu-latest
    if: github.event_name == 'push' || github.event_name == 'workflow_dispatch'
    steps:
      - name: Configure AWS credentials
        uses: aws-actions/configure-aws-credentials@v2
        with:
          aws-access-key-id: ${{ secrets.AWS_ACCESS_KEY_ID }}
          aws-secret-access-key: ${{ secrets.AWS_SECRET_ACCESS_KEY }}
          aws-region: ${{ secrets.AWS_REGION }}

      - name: Install AWS CLI
        run: |
          pip install awscli jq

      - name: Create and Upload Test File
        run: |
          echo '{"message": "Test from GitHub Actions", "timestamp": "'$(date -u +"%Y-%m-%dT%H:%M:%SZ")'"}' > sample.txt
          cat sample.txt
          aws s3 cp sample.txt s3://${{ needs.deploy.outputs.bucket_name }}/sample.txt
          echo "Test file uploaded to S3 bucket"

      - name: Wait for Lambda Execution
        run: |
          echo "Waiting for Lambda to process the file..."
          sleep 10  # Give Lambda some time to process

      - name: Check Lambda Logs
        id: check-logs
        run: |
          # Create log group name from Lambda function name
          LOG_GROUP_NAME="/aws/lambda/${{ needs.deploy.outputs.lambda_name }}"
          echo "Checking log group: $LOG_GROUP_NAME"
          
          # Get the latest log stream
          LOG_STREAM=$(aws logs describe-log-streams \
            --log-group-name "$LOG_GROUP_NAME" \
            --order-by LastEventTime \
            --descending \
            --limit 1 \
            --query 'logStreams[0].logStreamName' \
            --output text)
          
          echo "Latest log stream: $LOG_STREAM"
          
          # Check logs for successful processing
          CURRENT_TIME=$(date +%s)
          START_TIME=$((CURRENT_TIME - 300))  # 5 minutes ago
          END_TIME=$CURRENT_TIME
          
          # Get logs and search for successful processing
          LOGS=$(aws logs get-log-events \
            --log-group-name "$LOG_GROUP_NAME" \
            --log-stream-name "$LOG_STREAM" \
            --start-time $((START_TIME * 1000)) \
            --end-time $((END_TIME * 1000)))
          
          echo "Recent logs:"
          echo "$LOGS" | jq -r '.events[].message'
          
          # Check if processing was successful
          if echo "$LOGS" | grep -q "Processed content"; then
            echo "✅ Lambda function processed the file successfully!"
            echo "success=true" >> $GITHUB_OUTPUT
          else
            echo "❌ Lambda function did not process the file or encountered an error."
            echo "success=false" >> $GITHUB_OUTPUT
          fi

      - name: Verify Lambda Execution
        run: |
          if [ "${{ steps.check-logs.outputs.success }}" == "true" ]; then
            echo "✅ End-to-end test passed! Lambda successfully processed the uploaded file."
          else
            echo "❌ End-to-end test failed! Lambda did not process the file correctly."
            exit 1
          fi

  destroy:
    name: Destroy Resources
    needs: [deploy, verify]
    runs-on: ubuntu-latest
    if: always() # Run even if verify job failed
    steps:
      - name: Checkout code
        uses: actions/checkout@v3

      - name: Setup Node.js and Python
        uses: actions/setup-node@v3
        with:
          node-version: '18'
      
      - name: Setup Python
        uses: actions/setup-python@v4
        with:
          python-version: '3.12'

      - name: Install dependencies
        run: |
          python -m pip install --upgrade pip
          pip install -r requirements.txt
          npm install -g aws-cdk
          pip install awscli

      - name: Configure AWS credentials
        uses: aws-actions/configure-aws-credentials@v2
        with:
          aws-access-key-id: ${{ secrets.AWS_ACCESS_KEY_ID }}
          aws-secret-access-key: ${{ secrets.AWS_SECRET_ACCESS_KEY }}
          aws-region: ${{ secrets.AWS_REGION }}

      - name: Cleanup Resources
        run: |
          echo "Cleaning up resources..."
          # Empty the S3 bucket first to avoid deletion failures
          aws s3 rm s3://${{ needs.deploy.outputs.bucket_name }} --recursive || true
          # Destroy the CDK stack
          cdk destroy --force
          echo "✅ Resources destroyed successfully"